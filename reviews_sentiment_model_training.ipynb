{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Init</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Prep</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep():\n",
    "  amazon = pd.read_csv('./files/amazon_cells_labelled.txt', sep='\\t', encoding='ISO-8859-1', on_bad_lines='skip', names=['Review', 'Label'])\n",
    "  imdb = pd.read_csv('./files/imdb_labelled.txt', sep='\\t', encoding='ISO-8859-1', on_bad_lines='skip', names=['Review', 'Label'])\n",
    "  yelp = pd.read_csv('./files/yelp_labelled.txt', sep='\\t', encoding='ISO-8859-1', on_bad_lines='skip', names=['Review', 'Label'])\n",
    "  reviews_df = pd.concat([amazon, imdb, yelp], axis=0, ignore_index=True)\n",
    "\n",
    "  stop_words = stopwords.words('english')\n",
    "  stop_words.extend(['.', ',', \"'\", '\"', '?', '!', '-', '/', ':', '(', ')', '\\n', '@'])\n",
    "  to_remove = [\"but\", \"or\", \"against\", \"on\", \"off\", \"both\", \"no\", \"nor\", \"not\", \"only\", \"same\", \"don'\", \"don't\", \"ain'\", \"aren'\", \"aren't\", \"could'\", \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'wouldn', \"wouldn't\", 'ain', 'aren', 'couldn']\n",
    "  stop_words = [word for word in stop_words if word not in to_remove]\n",
    "  sent_list = reviews_df['Review'].apply(sent_tokenize).tolist()\n",
    "  reviews_list = [' '.join(inner_list) for inner_list in sent_list]\n",
    "\n",
    "  return reviews_df, stop_words, reviews_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(stop_words, reviews_list):\n",
    "    filtered_reviews = []\n",
    "    for sentence in reviews_list:\n",
    "        words = WordPunctTokenizer().tokenize(sentence)\n",
    "        filtered_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower not in stop_words:\n",
    "                pos = pos_tag([word])[0][1]\n",
    "                \n",
    "                if word_lower.endswith('ness'):  # Nouns\n",
    "                    stemmer = LancasterStemmer()\n",
    "                    filtered_word = stemmer.stem(word_lower)\n",
    "                elif pos.startswith('VB'):  # Verbs\n",
    "                    lemmatizer = WordNetLemmatizer()\n",
    "                    filtered_word = lemmatizer.lemmatize(word_lower, pos='v')\n",
    "                else:\n",
    "                    filtered_word = word_lower\n",
    "                \n",
    "                filtered_words.append(filtered_word)\n",
    "        \n",
    "        filtered_sentence = \" \".join(filtered_words)\n",
    "        filtered_reviews.append(filtered_sentence)\n",
    "    return filtered_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Vectorize</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(reviews):\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  f_vectors = vectorizer.fit(reviews)\n",
    "  vectorized_reviews = f_vectors.transform(reviews)\n",
    "  return pd.DataFrame(vectorized_reviews.toarray(), columns=f_vectors.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Define | Split X and Y</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_split_x_y(vectorized_reviews, labels):\n",
    "    vectorized_reviews['labels'] = labels\n",
    "\n",
    "    x = vectorized_reviews.drop(['labels'], axis=1)\n",
    "    y = vectorized_reviews['labels']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Train | Predict</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(x_train, x_test, y_train):\n",
    "  xgb_c = xgb.XGBClassifier()\n",
    "  xgb_model = xgb_c.fit(X=x_train, y=y_train)\n",
    "  x_test_pred = xgb_model.predict(x_test)\n",
    "  return xgb_model, x_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Pickle</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickles(stop_words, xgb_model):\n",
    "  pickle.dump(stop_words, open('./files/stop_words.pickle', 'wb'))\n",
    "  pickle.dump(xgb_model, open('./files/xgb_model.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>Evaluate</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, x_test_pred):\n",
    "  return roc_auc_score(y_test, x_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <center>FN Call</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df, stop_words, reviews_list = prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews = preprocess_reviews(stop_words, reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_reviews = vectorize(filtered_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = define_split_x_y(vectorized_reviews, reviews_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model, x_test_pred = train_predict(x_train, x_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles(stop_words, xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_score = evaluate(y_test, x_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907927976806091"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
